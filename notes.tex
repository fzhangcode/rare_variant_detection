\documentclass[10pt, letterpaper]{article}

\usepackage{amsmath} % just math
\usepackage{amssymb} % allow blackboard bold (aka N,R,Q sets)
\linespread{1.6}  % double spaces lines
\usepackage[left=1in,top=1in,right=1in,bottom=1in,nohead]{geometry}

\newcommand{\E}{\mathbb{E}}

\begin{document}

\linespread{1} % single spaces lines
\small \normalsize %% dumb, but have to do this for the prev to work
\begin{flushright}
RVD2 Project Notes \\
\today
\end{flushright}

Consider the following model
\begin{align}
\mu_j &\thicksim \text{Gaussian}(u_0,\sigma^2_0)\\
\theta_{kj} &\thicksim \text{Beta}(\mu_j,M_0)\\
r_{kj} &\thicksim \text{Bino}(\theta_{kj}, n_{kj})
\end{align}

The inferential object of interest is the joint posterior distribution over the latent variables, $p(\mu,\theta|r)$. From this we can compute the marginal posterior distribution over the site-specific error rate - the null hypothesis distribution.
\begin{equation}
p(\mu|r) = \int_\theta p(\mu,\theta|r)d\theta
\end{equation}

Writing out the joint posterior distribution gives
\begin{equation}
\begin{split}
\log p(\mu,\theta | r)& = \log \frac{p(\mu,\theta,r;u_o,\sigma^2_0,M_0)}{p(r | u_o,\sigma^2_0,M_0)}\\
& = \log p(\mu,\theta,r;u_o,\sigma^2_0,M_0) - \log p(r ; u_o, \sigma^2_0,M_0)\\
& = \log \prod_{j=1}^J \prod_{k=1}^K p(\mu_j,\theta_{kj}, r_{kj}) - \log \prod_{j=1}^J \prod_{k=1}^K \int_{\theta_{kj}} \int_{\mu_j} p(\mu_j,\theta_{kj}, r_{kj})d\mu_jd\theta_{kj}
\end{split}
\end{equation}

\begin{multline}
\log p(\mu,\theta | r) = K\sum_{j=1}^J \log p(\mu_j;\mu_0,\sigma^2_0) + \sum_{j=1}^J \sum_{k=1}^K \log p(\theta_{kj} | \mu_j;M_0)\\ + \sum_{j=1}^J \sum_{k=1}^K\log p(r_{kj} | \theta_{kj}, n_{kj})
- \sum_{j=1}^J \sum_{k=1}^K \log \int_{\theta_{kj}} \int_{\mu_j} p(r_{kj};\mu_0,\sigma^2_0,M_0) \end{multline}

The computation of $p(r;u_0,\sigma_0^2,M_0)$ is intractable due to the coupling in the latent variables induced by the integral so we must use an approximation or sampling method.

{\bf Variational approximation}
\begin{align}
\log p(r_{kj}; u_0, \sigma_0^2, M_0) & = \log \int_{\theta_{kj}} \int_{\mu_j} p(\mu_j, \theta_{kj}, r_{kj})d\mu_jd\theta_{kj} \\
& = \log \int_{\theta_{kj}} \int_{\mu_j} p(\mu_j, \theta_{kj}, r_{kj}) \frac{q(\mu_j, \theta_{kj})}{q(\mu_j, \theta_{kj})}  d\mu_jd\theta_{kj}\\
& \geq  \int_{\theta_{kj}} \int_{\mu_j} q(\mu_j, \theta_{kj}) \log p(\mu_j, \theta_{kj}, r_{kj}) -  \int_{\theta_{kj}} \int_{\mu_j} q(\mu_j, \theta_{kj}) \log q(\mu_j, \theta_{kj}) \\
& = \E_q \left[ \log p(r_{kj}, \mu_j, \theta_{kj}) \right] - \E_q\left[ \log q(\mu_j, \theta_{kj}) \right] = \mathcal{L_{kj}}
\end{align}

The bound on the log likelihood of the data, $\mathcal{L}$, is seen to be the sum of the expected complete data log-likelihood and the entropy of the variational approximating distribution.

We now specify an averaging(variational) distribution
\begin{align}
q(\mu_j | \gamma_{1j}, \gamma_{2j}) & \thicksim \text{Gaussian}(\gamma_{1j}, \gamma_{2j})\\
q(\theta_{kj} | \alpha_{kj}, \beta_{kj} ) & \thicksim \text{Beta}(\alpha_{kj}, \beta_{kj})
\end{align}

We are now able to decompose $\mathcal{L}$ into its constituent parts
\begin{multline}\label{eqn:composite}
\mathcal{L} = K\sum_{j=1}^J \E_q\log p(\mu_j|u_0,\sigma_0^2) + \sum_{j=1}^J\sum_{k=1}^K\E_q \log p(\theta_{kj} | \mu_j; M_0) 
+ \sum_{j=1}^J\sum_{k=1}^K\E_q \log p(r_{kj} | \theta_{kj}, n_{kj})\\
- K\sum_{j=1}^J\E_q \log q(\mu) - \sum_{j=1}^J\sum_{k=1}^K \E_q \log q(\theta_{kj})
\end{multline}

Taking each term in \eqref{eqn:composite} individually we can cast the bound on the log-likelihood in terms of original and variational parameters only.
\begin{align}
\E_q \log p(\mu_j ; u_0, \sigma_0^2) & = -\frac{1}{2}\log (2\pi\sigma_0^2) -\frac{1}{2\sigma_0^2}\left[ \gamma_{2j} + (\gamma_{1j} - u_0)^2 \right]
\end{align}

\begin{align}
\E_q \log p(\theta_{kj} | \mu_j ; M_0) & = \int_{\mu_j} q(\mu_j) \int_{\theta_{kj}} q(\theta_{kj}) \log p(\theta_{kj} | \mu_j, M_0)\\
\begin{split}
& = \log \Gamma(M_0) - \E_q \log \Gamma(\mu_j M_0) - \E_q \log \Gamma((1-\mu_j)M_0) \\
& \qquad+(\gamma_{1j}M_0-1)\left( \psi(\alpha_{kj}) - \psi(\alpha_{kj} + \beta_{kj}) \right) \\
& \qquad + ((1-\gamma_{1j})M_0-1)\left( \psi(\beta_{kj}) - \psi(\alpha_{kj} + \beta_{kj}) \right)
\end{split}
\end{align}
where $\E_q \log \Gamma(\mu_j M_0)$ is easily computed numerically. In general, we will be able to differentiate under the integral by Leibniz integral rule and the fact that the bounds are constants $\mu_j \in [0,1]$.

\begin{align}
\E_q \log p(r_{kj} | \theta_{kj}, n_{kj}) & = \int_{\theta_{kj}} q(\theta_{kj}) \log p(r_{kj} | \theta_{kj}, n_{kj})\\
\begin{split}
& = \log \Gamma(n_{kj}+1) - \log \Gamma(r_{kj}+1) - \log \Gamma(n_{kj}-r_{kj}+1) \\
& \qquad +r(\psi(\alpha_{kj})-\psi(\alpha_{kj} + \beta_{kj})) +(n_{kj}-r_{kj})(\psi(\beta_{kj}) - \psi(\alpha_{kj}+\beta_{kj}))
\end{split}
\end{align}

\begin{equation}
\E_q \log q(\mu_j)  = -\frac{1}{2} \log(2\pi e \gamma_{2j})
\end{equation}

\begin{multline}
\E_q \log q(\theta_{kj})  = \log \Gamma(\alpha_{kj} + \beta_{kj}) - \log \Gamma(\alpha_{kj}) - \log \Gamma(\beta_{kj}) \\+(\alpha_{kj}-1)\psi(\alpha_{kj}) +(\beta_{kj} -1)\psi(\beta_{kj}) -(\alpha_{kj}+\beta_{kj}-2)\psi(\alpha_{kj}+\beta_{kj})
\end{multline}

Now that \eqref{eqn:composite} has been written in terms of only the variational and model parameters, we can numerically optimize the system with respect to variational and the model parameters and do coordinate ascent. Maximizing with respect to the variational parameters tightens the bound on the data log-likelihood and maximizing with respect to the model parameters maximizes the likelihood.

{\bf Update Equation for $u_0$ }

Isolating the terms of \eqref{eqn:composite} that involve $u_0$ gives
\begin{equation}
\mathcal{L}_{[u_0]} = K\sum_{j=1}^J\left(-\frac{1}{2\sigma_0^2}(\gamma_{1j}-u_0)^2\right)
\end{equation}

Taking the derivative and solving for $u_0$ gives
\begin{equation}
u_0 \leftarrow \frac{1}{J}\sum_{j=1}^J \gamma_{1j} 
\end{equation}

{\bf Update Equation for $\sigma_0^2$ }

Isolating the terms of \eqref{eqn:composite} that involve $\sigma^2_0$ gives
\begin{equation}
\mathcal{L}_{[\sigma^2_0]} = -\frac{J}{2}\log(2\pi\sigma^2_0) -\frac{1}{2\sigma_0^2}\sum_{j=1}^J\left(\gamma_{2j} + (\gamma_{1j}-u_0)^2 \right)
\end{equation}

Taking the derivative with respect to $\sigma^2_0$ gives
\begin{equation}
\frac{\partial \mathcal{L}}{\partial \sigma^2_0} = -\frac{J}{2\sigma^2_0} +\frac{1}{\sigma^4_0}\sum_{j=1}^J\left(\gamma_{2j} + (\gamma_{1j}-u_0)^2\right)
\end{equation}

Setting the derivative equal to zero and solving for $\sigma^2_0$ gives
\begin{equation}
\sigma^2_0 \leftarrow \frac{1}{J}\sum_{j=1}^J\left(\gamma_{2j} + (\gamma_{1j}-u_0)^2\right)
\end{equation}

{\bf Update Equation for $\alpha_{kj}$ and $\beta_{kj}$ }

Isolating terms that involve $\alpha_{kj}$ gives
\begin{multline}
\mathcal{L}_{[\alpha_{kj}]} = (u_0M_0 -1)(\psi(\alpha_{kj}) - \psi(\alpha_{kj}+\beta_{kj})) -((1-u_0)M_0-1)\psi(\alpha_{kj}+\beta_{kj}) \\
+ r_{kj}\psi(\alpha_{kj})-n_{kj}\psi(\alpha_{kj}+\beta_{kj}) \\
 - \log \Gamma (\alpha_{kj}+\beta_{kj}) + \log \Gamma(\alpha_{kj}) -(\alpha_{kj}-1)\psi(\alpha_{kj})+(\alpha_{kj}+\beta_{kj}-2)\psi(\alpha_{kj}+\beta_{kj})
\end{multline}

Setting the derivative equal to zero and solving for $\alpha_{kj}$ gives
\begin{equation}
 \frac{\partial \mathcal{L}}{\partial \alpha_{kj}} = \psi_1(\alpha_{kj})(u_0M_0+r-\alpha_{kj}) - \psi_1(\alpha_{kj}+\beta_{kj})(M_0+n-(\alpha_{kj}+\beta_{kj}))
\end{equation}

A similar procedure holds for $\beta_{kj}$ and the resulting update equation bears symmetry to that for $\alpha_{kj}$
\begin{equation}
\beta_{kj} \leftarrow \psi_1(\beta_{kj})\left[(1-u_0)M_0-1+(n-r)-\beta_{kj}\right] - \psi_1(\alpha_{kj}+\beta_{kj})\left[M_0+n-(\alpha_{kj}+\beta_{kj})\right]
\end{equation}

{\bf Update Equation for $M_0$ }

Isolating terms that involve $M_0$ gives
\begin{multline}
\mathcal{L}_{[M_0]} = \log \Gamma(M_0) -\E_q\log \Gamma(\mu_jM_0) -\E_q\log \Gamma((1-\mu_j)M_0) +\\
+\gamma_{1j}M_0(\psi(\alpha_{kj})-\psi(\alpha_{kj}+\beta_{kj})) +(1-\gamma_{1j})M_0(\psi(\beta_{kj})-\psi(\alpha_{kj}+\beta_{kj}))
\end{multline}

{\bf Update Equation for $\gamma_{2j}$ }

Isolating terms that involve $\gamma_{2j}$ gives
\begin{equation}
\mathcal{L}_{[\gamma_{2j}]} = -\frac{K}{2\sigma_0^2}\gamma_{2j} -\E_q\log\Gamma(\mu_jM_0) - \E_q\log\Gamma((1-\mu_j)M_0)+\frac{1}{2}\log(2\pi e \gamma_{2j})
\end{equation}

Taking the derivative with respect to $\gamma_{2j}$ gives
\begin{equation}\label{eqn:gam2_deriv}
\frac{\partial \mathcal{L}}{\partial \gamma_{2j}} = -\frac{K}{2\sigma_0^2} -\frac{\partial}{\partial \gamma_{2j}}\E_q\log\Gamma(\mu_jM_0) -\frac{\partial}{\partial \gamma_{2j}}\E_q\log\Gamma((1-\mu_j)M_0) +\frac{1}{2\gamma_{2j}}
\end{equation}

The two partials are very similar, so we show only the derivation of the first and apply the solution to the second replacing $\mu_j$ with $(1-\mu_j)$ in the gamma function.
\begin{align}\label{eqn:gam2_int1}
\frac{\partial}{\partial \gamma_{2j}}\E_q\log\Gamma(\mu_jM_0) &= \frac{\partial}{\partial \gamma_{2j}} \int_0^1 (2\pi\gamma_{2j})^{-1/2}\exp\left(-\frac{1}{2\gamma_{j2}}(\mu_j-\gamma_{1j})^2\right) \log\Gamma(\mu_jM_0)d\mu_j \\
& = \frac{1}{2\gamma_{2j}}\left(\frac{1}{\gamma_{2j}} \E_q \left[ (\mu_j-\gamma_{1j})^2 \log\Gamma(\mu_jM_0) \right] - \E_q \left[ \log\Gamma(\mu_jM_0) \right] \right)
\end{align}

Replacing \eqref{eqn:gam2_int1} in \eqref{eqn:gam2_deriv} gives
\begin{multline}
\frac{\partial \mathcal{L}}{\partial \gamma_{2j}} = -\frac{K}{2\sigma_0^2} 
- \frac{1}{2\gamma_{2j}}\left(\frac{1}{\gamma_{2j}} \E_q \left[ (\mu_j-\gamma_{1j})^2 \log\Gamma(\mu_jM_0) \right] - \E_q \left[ \log\Gamma(\mu_jM_0) \right] \right) \\
- \frac{1}{2\gamma_{2j}}\left(\frac{1}{\gamma_{2j}} \E_q \left[ (\mu_j-\gamma_{1j})^2 \log\Gamma((1-\mu_j)M_0) \right] - \E_q \left[ \log\Gamma((1-\mu_j)M_0) \right] \right) +\frac{1}{2\gamma_{2j}}
\end{multline}
Solving for the root gives the maximizing value of $\gamma_{2j}$ and the update.

\end{document}