\documentclass[10pt, letterpaper]{article}

\usepackage{amsmath} % just math
\usepackage{amssymb} % allow blackboard bold (aka N,R,Q sets)

\usepackage[boxed]{algorithm2e}

\linespread{1.6}  % double spaces lines
\usepackage[left=1in,top=1in,right=1in,bottom=1in,nohead]{geometry}

\newcommand{\E}{\mathbb{E}}

\begin{document}

\linespread{1} % single spaces lines
\small \normalsize %% dumb, but have to do this for the prev to work
\begin{flushright}
RVD2 Project Notes \\
\today
\end{flushright}

\section*{RVD2.1: Independent variational parameters}

Consider the following model
\begin{align}
\mu_j &\thicksim \text{Gaussian}(u_0,\sigma^2_0)\\
\theta_{kj} &\thicksim \text{Beta}(\mu_j,M_0)\\
r_{kj} &\thicksim \text{Bino}(\theta_{kj}, n_{kj})
\end{align}

The inferential object of interest is the joint posterior distribution over the latent variables, $p(\mu,\theta|r)$. From this we can compute the marginal posterior distribution over the site-specific error rate - the null hypothesis distribution.
\begin{equation}
p(\mu|r) = \int_\theta p(\mu,\theta|r)d\theta
\end{equation}

Writing out the joint posterior distribution gives
\begin{equation}
\begin{split}
\log p(\mu,\theta | r)& = \log \frac{p(\mu,\theta,r;u_o,\sigma^2_0,M_0)}{p(r | u_o,\sigma^2_0,M_0)}\\
& = \log p(\mu,\theta,r;u_o,\sigma^2_0,M_0) - \log p(r ; u_o, \sigma^2_0,M_0)\\
& = \log \prod_{j=1}^J \prod_{k=1}^K p(\mu_j,\theta_{kj}, r_{kj}) - \log \prod_{j=1}^J \prod_{k=1}^K \int_{\theta_{kj}} \int_{\mu_j} p(\mu_j,\theta_{kj}, r_{kj})d\mu_jd\theta_{kj}
\end{split}
\end{equation}

\begin{multline}
\log p(\mu,\theta | r) = K\sum_{j=1}^J \log p(\mu_j;\mu_0,\sigma^2_0) 
	+\sum_{j=1}^J \sum_{k=1}^K \log p(\theta_{kj} | \mu_j;M_0)\\ 
	+\sum_{j=1}^J \sum_{k=1}^K\log p(r_{kj} | \theta_{kj}, n_{kj})
	- \sum_{j=1}^J \sum_{k=1}^K \log \int_{\theta_{kj}} \int_{\mu_j} p(r_{kj};\mu_0,\sigma^2_0,M_0) 
\end{multline}

The computation of $p(r;u_0,\sigma_0^2,M_0)$ is intractable due to the coupling in the latent variables induced by the integral so we must use an approximation or sampling method.

{\bf Variational approximation}
\begin{align}
\log p(r_{kj}; u_0, \sigma_0^2, M_0) & = \log \int_{\theta_{kj}} \int_{\mu_j} p(\mu_j, \theta_{kj}, r_{kj})d\mu_jd\theta_{kj} \\
& = \log \int_{\theta_{kj}} \int_{\mu_j} p(\mu_j, \theta_{kj}, r_{kj}) \frac{q(\mu_j, \theta_{kj})}{q(\mu_j, \theta_{kj})}  d\mu_jd\theta_{kj}\\
& \geq  \int_{\theta_{kj}} \int_{\mu_j} q(\mu_j, \theta_{kj}) \log p(\mu_j, \theta_{kj}, r_{kj}) -  \int_{\theta_{kj}} \int_{\mu_j} q(\mu_j, \theta_{kj}) \log q(\mu_j, \theta_{kj}) \\
& = \E_q \left[ \log p(r_{kj}, \mu_j, \theta_{kj}) \right] - \E_q\left[ \log q(\mu_j, \theta_{kj}) \right] = \mathcal{L}_{kj}
\end{align}

The bound on the log likelihood of the data, $\mathcal{L}$, is seen to be the sum of the expected complete data log-likelihood and the entropy of the variational approximating distribution.

We now specify an averaging(variational) distribution
\begin{align}
q(\mu_j | \gamma_{1j}, \gamma_{2j}) & \thicksim \text{Gaussian}(\gamma_{1j}, \gamma_{2j})\\
q(\theta_{kj} | \alpha_{kj}, \beta_{kj} ) & \thicksim \text{Beta}(\alpha_{kj}, \beta_{kj})
\end{align}

%%%			log-likelihood bound			%%%
We are now able to decompose $\mathcal{L}$ into its constituent parts
\begin{multline}\label{eqn:composite}
\mathcal{L} = K\sum_{j=1}^J \E_q\log p(\mu_j|u_0,\sigma_0^2) + \sum_{j=1}^J\sum_{k=1}^K\E_q \log p(\theta_{kj} | \mu_j; M_0) 
+ \sum_{j=1}^J\sum_{k=1}^K\E_q \log p(r_{kj} | \theta_{kj}, n_{kj})\\
- K\sum_{j=1}^J\E_q \log q(\mu) - \sum_{j=1}^J\sum_{k=1}^K \E_q \log q(\theta_{kj})
\end{multline}

Taking each term in \eqref{eqn:composite} individually we can cast the bound on the log-likelihood in terms of original and variational parameters only.
\begin{align}
\E_q \log p(\mu_j ; u_0, \sigma_0^2) & = -\frac{1}{2}\log (2\pi\sigma_0^2) -\frac{1}{2\sigma_0^2}\left[ \gamma_{2j} + (\gamma_{1j} - u_0)^2 \right]
\end{align}

\begin{align}
\E_q \log p(\theta_{kj} | \mu_j ; M_0) & = \int_{\mu_j} q(\mu_j) \int_{\theta_{kj}} q(\theta_{kj}) \log p(\theta_{kj} | \mu_j, M_0) \nonumber\\
\begin{split}
& = \log \Gamma(M_0) -\E_q \log \Gamma(\mu_j M_0) -\E_q \log \Gamma((1-\mu_j)M_0) \\
& \qquad+(\gamma_{1j}M_0-1)\left( \psi(\alpha_{kj}) - \psi(\alpha_{kj} + \beta_{kj}) \right) \\
& \qquad + ((1-\gamma_{1j})M_0-1)\left( \psi(\beta_{kj}) - \psi(\alpha_{kj} + \beta_{kj}) \right)
\end{split}
\end{align}
where $\E_q \log \Gamma(\mu_j M_0)$ is easily computed numerically. In general, we will be able to differentiate under the integral by Leibniz integral rule and the fact that the bounds are constants $\mu_j \in [0,1]$.

\begin{align}
\E_q \log p(r_{kj} | \theta_{kj}, n_{kj}) & = \int_{\theta_{kj}} q(\theta_{kj}) \log p(r_{kj} | \theta_{kj}, n_{kj}) \nonumber \\
\begin{split}
& = \log \Gamma(n_{kj}+1) - \log \Gamma(r_{kj}+1) - \log \Gamma(n_{kj}-r_{kj}+1) \\
& \qquad +r(\psi(\alpha_{kj})-\psi(\alpha_{kj} + \beta_{kj})) +(n_{kj}-r_{kj})(\psi(\beta_{kj}) - \psi(\alpha_{kj}+\beta_{kj}))
\end{split}
\end{align}

\begin{equation}
\E_q \log q(\mu_j)  = -\frac{1}{2} \log(2\pi e \gamma_{2j})
\end{equation}

\begin{multline}
\E_q \log q(\theta_{kj})  = \log \Gamma(\alpha_{kj} + \beta_{kj}) - \log \Gamma(\alpha_{kj}) - \log \Gamma(\beta_{kj}) \\+(\alpha_{kj}-1)\psi(\alpha_{kj}) +(\beta_{kj} -1)\psi(\beta_{kj}) -(\alpha_{kj}+\beta_{kj}-2)\psi(\alpha_{kj}+\beta_{kj})
\end{multline}

Now that \eqref{eqn:composite} has been written in terms of only the variational and model parameters, we can numerically optimize the system with respect to variational and the model parameters and do coordinate ascent. Maximizing with respect to the variational parameters tightens the bound on the data log-likelihood and maximizing with respect to the model parameters maximizes the likelihood.

%%%			u0 update				%%%
{\bf Update Equation for $u_0$ }

Isolating the terms of \eqref{eqn:composite} that involve $u_0$ gives
\begin{equation*}
\mathcal{L}_{[u_0]} = K\sum_{j=1}^J\left(-\frac{1}{2\sigma_0^2}(\gamma_{1j}-u_0)^2\right)
\end{equation*}

Taking the derivative and solving for $u_0$ gives
\begin{equation}
u_0 \leftarrow \frac{1}{J}\sum_{j=1}^J \gamma_{1j} 
\end{equation}

%%%			sigma2 update			%%%
{\bf Update Equation for $\sigma_0^2$ }

Isolating the terms of \eqref{eqn:composite} that involve $\sigma^2_0$ gives
\begin{equation*}
\mathcal{L}_{[\sigma^2_0]} = -\frac{J}{2}\log(2\pi\sigma^2_0) -\frac{1}{2\sigma_0^2}\sum_{j=1}^J\left(\gamma_{2j} + (\gamma_{1j}-u_0)^2 \right)
\end{equation*}

Taking the derivative with respect to $\sigma^2_0$ gives
\begin{equation*}
\frac{\partial \mathcal{L}}{\partial \sigma^2_0} = -\frac{J}{2\sigma^2_0} +\frac{1}{\sigma^4_0}\sum_{j=1}^J\left(\gamma_{2j} + (\gamma_{1j}-u_0)^2\right)
\end{equation*}

Setting the derivative equal to zero and solving for $\sigma^2_0$ gives
\begin{equation}
\sigma^2_0 \leftarrow \frac{1}{J}\sum_{j=1}^J\left(\gamma_{2j} + (\gamma_{1j}-u_0)^2\right)
\end{equation}

%%% 		M0 update 		%%%
{\bf Update Equation for $M_0$ }

Isolating terms that involve $M_0$ gives
\begin{multline*}
\mathcal{L}_{[M_0]} = \log \Gamma(M_0) -\E_q\log \Gamma(\mu_jM_0) -\E_q\log \Gamma((1-\mu_j)M_0) +\\
+\gamma_{1j}M_0(\psi(\alpha_{kj})-\psi(\alpha_{kj}+\beta_{kj})) +(1-\gamma_{1j})M_0(\psi(\beta_{kj})-\psi(\alpha_{kj}+\beta_{kj}))
\end{multline*}

Taking the derivative with respect to $M_0$ gives
\begin{multline*}
\frac{\partial \mathcal{L}}{\partial M_0} = \psi(M_0) -\frac{\partial}{\partial M_0}\E_q\log \Gamma(\mu_jM_0)
-\frac{\partial}{\partial M_0}\E_q\log \Gamma((1-\mu_j)M_0) \\
+\gamma_{1j}(\psi(\alpha_{kj})-\psi(\alpha_{kj}+\beta_{kj})) 
+(1-\gamma_{1j})(\psi(\beta_{kj})-\psi(\alpha_{kj}+\beta_{kj}))
\end{multline*}

The partial derivative can be taken inside of the integral which gives $\frac{\partial}{\partial M_0}\E_q\log \Gamma(\mu_jM_0) = \E_q \left[ \mu_j \psi(\mu_j M_0) \right]$ and a similar expression for the second term $\frac{\partial}{\partial M_0}\E_q\log \Gamma((1-\mu_j)M_0) = \E_q \left[(1- \mu_j) \psi((1-\mu_j) M_0) \right]$.

Plugging these simplifications into the partial with respect to $M_0$ gives
\begin{multline}
\frac{\partial \mathcal{L}}{\partial M_0} = \psi(M_0) - \E_q \left[ \mu_j \psi(\mu_j M_0) \right]
-\E_q \left[(1- \mu_j) \psi((1-\mu_j) M_0) \right] \\
+\gamma_{1j}(\psi(\alpha_{kj})-\psi(\alpha_{kj}+\beta_{kj})) 
+(1-\gamma_{1j})(\psi(\beta_{kj})-\psi(\alpha_{kj}+\beta_{kj}))
\end{multline}

%%%		alpha & beta update			%%%
{\bf Update Equation for $\alpha_{kj}$ and $\beta_{kj}$ }

Isolating terms that involve $\alpha_{kj}$ gives
\begin{multline*}
\mathcal{L}_{[\alpha_{kj}]} = (u_0M_0 -1)(\psi(\alpha_{kj}) - \psi(\alpha_{kj}+\beta_{kj})) -((1-u_0)M_0-1)\psi(\alpha_{kj}+\beta_{kj}) \\
+ r_{kj}\psi(\alpha_{kj})-n_{kj}\psi(\alpha_{kj}+\beta_{kj}) \\
 - \log \Gamma (\alpha_{kj}+\beta_{kj}) + \log \Gamma(\alpha_{kj}) -(\alpha_{kj}-1)\psi(\alpha_{kj})+(\alpha_{kj}+\beta_{kj}-2)\psi(\alpha_{kj}+\beta_{kj})
\end{multline*}

Setting the derivative equal to zero and solving for $\alpha_{kj}$ gives
\begin{equation}
 \frac{\partial \mathcal{L}}{\partial \alpha_{kj}} = \psi_1(\alpha_{kj})(u_0M_0+r-\alpha_{kj}) - \psi_1(\alpha_{kj}+\beta_{kj})(M_0+n-(\alpha_{kj}+\beta_{kj}))
\end{equation}

A similar procedure holds for $\beta_{kj}$ and the resulting update equation bears symmetry to that for $\alpha_{kj}$,
\begin{equation}
 \frac{\partial \mathcal{L}}{\partial \beta_{kj}} = \psi_1(\beta_{kj}) \left[ (1-u_0)M_0-1+(n_{kj}-r_{kj})-\beta_{kj}\right] - \psi_1(\alpha_{kj}+\beta_{kj})(M_0+n-(\alpha_{kj}+\beta_{kj}))
\end{equation}

%%% 		gamma1 update 		%%%
{\bf Update equation for $\gamma_{1j}$}\label{sec:gamma1_update}

Isolating terms that involve $\gamma_{1j}$ gives
\begin{multline*}
\mathcal{L}_{[\gamma_{1j}]} = -\frac{K}{2\sigma_0^2}(\gamma_{1j}-u_0)^2 
-\E_q \log \Gamma(\mu_j M_0) -\E_q \log \Gamma((1-\mu_j)M_0) \\
+ \sum_{k=1}^K \gamma_{1j} M_0 ( \psi(\alpha_kj) - \psi(\alpha_{kj} + \beta_{kj}) )
- \sum_{k=1}^K \gamma_{1j} M_0 ( \psi(\beta_kj) - \psi(\alpha_{kj} + \beta_{kj}) )
\end{multline*}

Taking the derivative with respect to $\gamma_{1j}$  gives
\begin{multline}
\frac{\partial \mathcal{L}}{\partial \gamma_{1j}} = -\frac{K}{\sigma_0^2}(\gamma_{1j} - u_0) 
	+M_0 \sum_{k=1}^K \left( \psi(\alpha_{kj}) - \psi(\beta_{kj}) \right) \\
	-K \E_q \left[ \frac{1}{\gamma_{2j}} \log \Gamma(\mu_jM_0) \right]
	-K \E_q \left[ \frac{1}{\gamma_{2j}} \log \Gamma((1-\mu_j)M_0) \right]
\end{multline}

%%% 		gamma2 update 		%%%
{\bf Update Equation for $\gamma_{2j}$ }\label{sec:gamma2_update}

Isolating terms that involve $\gamma_{2j}$ gives
\begin{equation*}
\mathcal{L}_{[\gamma_{2j}]} = -\frac{K}{2\sigma_0^2}\gamma_{2j} -\E_q\log\Gamma(\mu_jM_0) - \E_q\log\Gamma((1-\mu_j)M_0)+\frac{1}{2}\log(2\pi e \gamma_{2j})
\end{equation*}

Taking the derivative with respect to $\gamma_{2j}$ gives
\begin{equation}\label{eqn:gam2_deriv}
\frac{\partial \mathcal{L}}{\partial \gamma_{2j}} = -\frac{K}{2\sigma_0^2} -\frac{\partial}{\partial \gamma_{2j}}\E_q\log\Gamma(\mu_jM_0) -\frac{\partial}{\partial \gamma_{2j}}\E_q\log\Gamma((1-\mu_j)M_0) +\frac{1}{2\gamma_{2j}}
\end{equation}

The two partials are very similar, so we show only the derivation of the first and apply the solution to the second replacing $\mu_j$ with $(1-\mu_j)$ in the gamma function.
\begin{align}\label{eqn:gam2_int1}
\frac{\partial}{\partial \gamma_{2j}}\E_q\log\Gamma(\mu_jM_0) &= \frac{\partial}{\partial \gamma_{2j}} \int_0^1 (2\pi\gamma_{2j})^{-1/2}\exp\left(-\frac{1}{2\gamma_{j2}}(\mu_j-\gamma_{1j})^2\right) \log\Gamma(\mu_jM_0)d\mu_j \\
& = \frac{1}{2\gamma_{2j}}\left(\frac{1}{\gamma_{2j}} \E_q \left[ (\mu_j-\gamma_{1j})^2 \log\Gamma(\mu_jM_0) \right] - \E_q \left[ \log\Gamma(\mu_jM_0) \right] \right)
\end{align}

Replacing \eqref{eqn:gam2_int1} in \eqref{eqn:gam2_deriv} gives
\begin{multline}
\frac{\partial \mathcal{L}}{\partial \gamma_{2j}} = -\frac{K}{2\sigma_0^2} 
- \frac{1}{2\gamma_{2j}}\left(\frac{1}{\gamma_{2j}} \E_q \left[ (\mu_j-\gamma_{1j})^2 \log\Gamma(\mu_jM_0) \right] - \E_q \left[ \log\Gamma(\mu_jM_0) \right] \right) \\
- \frac{1}{2\gamma_{2j}}\left(\frac{1}{\gamma_{2j}} \E_q \left[ (\mu_j-\gamma_{1j})^2 \log\Gamma((1-\mu_j)M_0) \right] - \E_q \left[ \log\Gamma((1-\mu_j)M_0) \right] \right) +\frac{1}{2\gamma_{2j}}
\end{multline}
Solving for the root gives the maximizing value of $\gamma_{2j}$ and the update.


\section{RVD 2.3: Gamma Prior}
Consider the following model
\begin{align}
\alpha_j &\thicksim \text{Gamma}(a, b)\\
\theta_{ij} &\thicksim \text{Dirichlet}(\alpha_j)\\
r_{ij} &\thicksim \text{Multinomial}(\theta_{ij}, n_{ij})
\end{align}

For each genomic location, $j=1,\ldots,M$, $K$ random variables are independently chosen from a Gamma distribution with shape parameter $a$ and scale parameter $b$. Within each location, $N$ Dirichlet random variables, $\theta_{ij}$ are chosen with parameter $\alpha_j$ for each replicate. Finally, a read count variable $r_{ij}$ is drawn from a multinomial distribution with probability parameter $\theta_{ij}$ and total count $n_{ij}$. The count of nucleotide $k\in\{A, C, T, G\}$ at position $j$ in replicate $i$ is then $r_{ij}^k$.

The inferential object of interest here is the joint posterior distribution over the latent variables $p(\theta, \alpha | r)$. From the posterior distribution we can estimate the location-specific Dirichlet parameters $\hat{\alpha}_j$ and thus the location-specific nucleotide probabilities $\frac{ \hat{\alpha}^k_{j} } { \hat{\alpha}^0_{j} }$, where $\hat{\alpha}^0_{j} = \sum_k \hat{\alpha}^k_j$.

The joint posterior distribution is
\begin{equation}
p(\alpha, \theta | r; a, b) = \prod_{i=1}^N \prod_{j=1}^M \frac{ p(r_{ij} | \theta_{ij}) p(\theta_{ij} | \alpha_j) p(\alpha_j; a, b) } { p(r_{ij} ; a, b) }.
\end{equation}
But, normalization factor in the posterior distribution,
\begin{equation}
p(r_{ij}; a, b) = \int_{\theta_{ij}} \int_{\alpha_j} p(r_{ij} | \theta_{ij}) p(\theta_{ij} | \alpha_j) p(\alpha_j; a, b),
\end{equation}
is computationally intractable due to the coupling between $\theta_{ij}$ and $\alpha_j$.

Instead of attempting to directly compute the posterior distribution, we instead sample it using  a Metropolis-Hastings within Gibbs scheme.

\begin{algorithm}[H]
\SetAlgoLined
\KwData{Counts $r_{ij}$ for experimental replicate $i$ at genomic location $j$}
\KwResult{MLE estimates for hyper-parameters $\hat{a}$ and $\hat{b}$. Samples from joint posterior distribution $p(\theta_{ij}, \alpha_j | r_{ij}; \hat{a}, \hat{b})$ for $i=1,\ldots,N$ and $j=1,\ldots,M$}
Initialize $a$ and $b$\;
\Repeat {convergence} {
	Sample $\alpha_j$ from $p(\alpha_j | a, b, \theta_{1j}, \ldots, \theta_{Nj})$ using Metropolis-Hastings\;
	Sample $\theta_{ij}$ from $p(\theta_{ij} | \alpha_j, r_{ij})$\;
	Compute maximum-likelihood estimates $\hat{a}$ and $\hat{b}$
}
\caption{Metropolis-within-Gibbs Sampling for RVD2.3 Model}
\end{algorithm}

The maximum-likelihood estimates $\hat{a}$ and $\hat{b}$ are computed using generalized Newton-Raphson as outlined in http://research.microsoft.com/en-us/um/people/minka/papers/minka-gamma.pdf

Samples from $p(\theta_{ij} | \alpha_j, r_{ij})$ can be obtained directly because the Dirichlet is conjugate to the Multinomial. The posterior sampling distribution is
\begin{equation}
	\theta_{ij} | \alpha_j, r_{ij} \thicksim \text{Dirichlet}(\alpha_j + r_{ij}).
\end{equation}

The Gamma distribution is not conjugate to the Dirichlet, so we draw samples from $p(\alpha_j | a, b, \theta_{1j}, \ldots, \theta_{Nj})$ by Metropolis-Hastings. We use a proposal distribution,
\begin{equation}
	\alpha_j^{k \text{(new)}} | \alpha_j^k \thicksim \text{Gaussian}(\alpha_j^k, 0.05),
\end{equation}
which we find mixes sufficiently fast to sample from the posterior after a burn-in of length X.
	
\end{document}