\documentclass[11pt,reqno]{amsart}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
%\usepackage{algorithm, algorithmic}

\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{graphicx}

\usepackage{verbatim}
\usepackage{amssymb}
\usepackage{amsmath}
%\usepackage{epstopdf}
%\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\newcommand{\RR}{I\!\!R} %real numbers
\DeclareMathOperator{\diag}{diag}

\title{RVD2: An ultra-sensitive variant detection model for heterogeneous population sequencing}
\author{}
%\date{}                                           % Activate to display a given date or no date

\begin{document}
\maketitle

\section{Introduction}

\section{Model structure}\

Given a data matrix $r \in \RR^{J \times N}$ where the element $r_{ji}$ is the count of the reads that contain a non-reference base at position $j$ in experimental replicate $i$. The total count of reads at position $i$ in replicate $i$ is $n_{ji}$. A typical data set is comprised of case and control data sets, $\{ r^{\text{case}} , n^{\text{case}} \}$ and  $\{ r^{\text{control}} , n^{\text{control}} \}$. 

We have developed a hierarchical Bayesian model for this data set because of the nested structure of noise in the data. There is sampling error due to the fact that only a finite number of reads aligned and mapped for each location. There is inter-replicate errors due to experimental protocol and reproducibility. Finally there is sequence context error due to the fidelity of the DNA polymerase enzymes in synthesizing a base. These errors have been shown to depend heavily on the neighboring DNA bases and whether the base is at the end of a molecule or at the center of a molecule.

\subsection{Generative Process} We have codified the statistical structure of these errors in the following generative process (we have used the mean-precision parameterized of the beta distribution): 

\begin{enumerate}
	\item Choose a baseline error rate parameter $\mu_0$ and a inter-location precision parameter $M_0$.
	\item For each location $j$:
	\begin{enumerate}
		\item Sample a location specific error rate parameter $\mu_j \thicksim \text{Beta}(\mu_0, M_0)$
		\item Choose an inter-replicate precision parameter $M_j$
		\item For each replicate $i$:
		\begin{enumerate}
			\item Sample an error rate $\theta_{ji} \thicksim \text{Beta}(\mu_j, M_j)$ for location $j$ in sequencing replicate $i$
			\item Sample the non-reference read count $r_{ji} \thicksim \text{Binomial}(\theta_{ji}, n_{ji})$
		\end{enumerate}
	\end{enumerate}
\end{enumerate}

This model has three levels of sampling. First the baseline error rate across all locations and all replicates is chosen once and for all for the entire data set. Then, based on that baseline error rate, location-specific error rates are sampled. Based on the location specific error rate, replicate-specific error rates are selected. Finally, the observable data, the count of the non-reference reads is sampled.

\begin{figure}[h]
\begin{center}
\includegraphics[width=40mm]{pdf_figs/RVD2_model.pdf}
\caption{RVD2 Graphical Model}
\label{fig:graphical_model}
\end{center}
\end{figure}

% Describe graphical model framework here

Given this model structure, the joint distribution over the latent and observed variables for data at location $j$ in replicate $i$ given the parameters is

\begin{equation}\label{eqn:jointpdf}
p \left( r_{ji}, \theta_{ji}, \mu_j | n_{ji}; \mu_0, M_0, M_j \right) = p \left( r_{ji} | \theta_{ji}, n_{ji} \right) p\left( \theta_{ji} | \mu_j; M_j \right) p\left( \mu_j; \mu_0, M_0 \right)
\end{equation}

\begin{gather}
= \frac{ \Gamma(M_0) } { \Gamma(\mu_0 M_0) \Gamma(M_0 (1-\mu_0)) } \mu_j^{M_0\mu_0 -1} (1 - \mu_j)^{M_0 ( 1 - \mu_0) - 1} \\
\cdot \frac{ \Gamma(M_j) } { \Gamma(\mu_j M_j) \Gamma(M_j (1-\mu_j)) } \theta_{ji}^{M_j\mu_j -1} (1 - \theta_{ji})^{M_j ( 1 - \mu_j) - 1} \\
\cdot \frac{ \Gamma(n_{ji}+1) } { \Gamma(r_{ji}+1) \Gamma( n_{ji} - r_{ji} + 1 ) } \theta_{ji}^{r_{ji}} (1 - \theta_{ji})^{n_{ji} - r_{ji}}
\end{gather}

Integrating over the latent variables $\theta_{ji}$ and $\mu_j$ yields the marginal distribution of the data at location $j$ in replicate $i$, 
\begin{equation}
p \left( r_{ji} | n_{ji} ; \mu_0, M_0, M_j \right) = \int_{\mu_j} \int_{\theta_{ji}}  p \left( r_{ji} | \theta_{ji}, n_{ji} \right) p\left( \theta_{ji} | \mu_j; M_j \right) p\left( \mu_j; \mu_0, M_0 \right) d\theta_{ji} d\mu_j
\end{equation}

Finally, assuming the conditional independence structure in the graphical model in Figure~\ref{fig:graphical_model}, the likelihood of the data set is

\begin{equation}
p \left( r ; \mu_0, M_0, M \right) = \prod_{j=1}^J p\left( \mu_j; \mu_0, M_0 \right) \prod_{i=1}^N p \left( r_{ji} | \theta_{ji}, n_{ji} \right) p\left( \theta_{ji} | \mu_j; M_j \right) 
\end{equation}

Instead of attempting to directly compute the posterior distribution, we instead sample it using a Metropolis-Hastings within Gibbs scheme as described in Metropolis-within-Gibbs section.

\section{Inference and Hypthesis Testing}
\subsection{Model parameter estimation}\

Method of moments is used to find the optimal model parameters and hyperparameters. From the feature of beta distribution
The estimate of 

The estimated error probability is calculated from the observed sample
\begin{equation}
 \theta_{ij}=\frac{r_{ij}}{n_{ij}}
\end{equation}

The estimate of parameter $\mu_j$ is obtained using the expected value of $\theta_{ij}$, while $M_j$ is estimated using the moment estimates for the variance of $\theta_{ij}$.
\begin{equation}
 \mu_j=E(\theta_{ij}|\mu_j,M_j)
\end{equation}
\begin{equation}
 M_j=\frac{\mu_j(1-\mu_j)}{Var(\theta_{ij}|\mu_j,M_j)}-1
\end{equation}

Likewise,the estimate of hyperparameters $\mu_o$ and $M_0$ are
\begin{equation}
 \mu_0=E(\mu_j|\mu_0,M_0)
\end{equation}
\begin{equation}
 M_0=\frac{\mu_0(1-\mu_0)}{Var(\mu_j|\mu_0,M_0)}-1
\end{equation}

\subsection{Metropolis-within-Gibbs sampling}\

Because of the computation complexities of the normalization factor in the posterior probability, A Markov Chain Monte Carlo (MCMC) sampling approach is employed to approximate the posterior distribution $p(\theta, \mu | r,n; \mu_0,M_0,M)$. Specifically Metropolis-within-Gibbs algorithm is used to obtain the posterior distribution of each latent variable.

\begin{algorithm}[ht]
\caption{Metropolis within Gibbs Algorithm}
\label{alg:metro_gibbs}
\begin{algorithmic}[1]
\Repeat
\For {j = 1 to J}
  \For {i =1 to N}
	\State Draw 1 samples from $p \left( \theta_{ij} |r_{ij},n_{ij},\mu_j,M \right)$ using Gibbs; sampling
  \EndFor
\EndFor

\For {j = 1 to J}
	\State Draw T samples from $p \left( \mu_j |\theta_{ij},\mu_0,M_0\right)$ using M--H;
	\State Set $mu_j$ to the sample median;
\EndFor
\Until {sample size is sufficient}

\end{algorithmic}
\end{algorithm}

\subsection{Statistic Inference}\

Full Bayesian Hypothesis Testing is done to test rare variants in the model.

\subsubsection{Posterior distribution difference sampling}\

Using MCMC sampling, we can obtain the posterior distribution $p \left( \mu |\theta,\mu_0,M_0\right)$ for reference sample and testing sample. In order to compare the difference of position error rate $\mu$ between reference sample and testing sample , a random sampling of posterior distribution is done 
\begin{algorithm}
\caption{Posterior distribution difference sampling}
\begin{algorithmic}[1]
\Procedure{}{}
\Repeat
\State  Randomly  draw one Gibbs sample from $p \left( \mu |r, n, M, \mu_0, M_0\right)$ from reference samples
\State  Randomly draw one Gibbs sample from $p \left( \mu |r, n, M, \mu_0,M_0\right) $ from testing samples
\State Subtract the former from the later to get one sample for the posterior difference distribution 
\Until {Sample size is sufficient}
\EndProcedure
\end{algorithmic}
\end{algorithm}

With the posterior difference distribution of position error rate $\mu$ across replicates obtained, Bayesian inference can be done to summarize the difference. 

$p \left( \mu_{testing}-\mu_{ref} |r,n; \mu_0,M_0\right)$

\subsubsection{Bayesian Hypothesis Testing}
To detect a mutation in a specific position, a threshold $\tau$ is set for the posterior difference distribution testing.
\begin{align}
 H_1: \mu_{testing}-\mu_{ref}\geq\tau \\
 H_2: \mu_{testing}-\mu_{ref}<\tau
\end{align}

We can calculate the posterior probability of the hypothesis $H_1$ being true by integrating the posterior density over the correct region:

\begin{equation}
 p(H1:\mu_{testing}-\mu_{ref}\geq\tau)=\int_\tau^\infty p(\mu_{diff} |r,n; \mu_0,M_0)d\mu_{diff}
\end{equation}

We accept the hypothesis $H_1$ if this posterior probability is no lower than 95\%, which means we are 95\% sure that there is a mutation in this position.

ROC curve is plotted to evaluate the performance of the detection algorithm with different threshold for different mutation rate. 

\subsubsection{Uniformity Testing}

\section{Result}
Figure 1 Overall of the process
\subsection{Simulation Results}
\subsection{Comparison Results on Synthetic DNA}
Figure 3 ROC comparison with other methods: varscan/samtools/rvd
\subsection{Empirical Results on Multiple Sclerosis Data}
We tested our method on sequence data from clinical Multiple Sclerosis samples. We expect to see that 

Table: Table of Variants
\subsection{Performance with read depth}
Figure 4 ROC by read depth
picard to thin segment data, showing performance as read depth decreasing


\bibliographystyle{apalike}
\bibliography{bioinfo}
\end{document}  